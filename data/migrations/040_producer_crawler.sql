-- Migration: 040_producer_crawler.sql
-- Purpose: Producer micro-crawler with RFC 9309 robots.txt governance
-- Created: 2026-01-15

-- ============================================================
-- UP MIGRATION
-- ============================================================

-- =============================================================================
-- Producer Domains (verified producer websites)
-- =============================================================================
CREATE TABLE IF NOT EXISTS producer_domains (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    domain TEXT NOT NULL UNIQUE,

    -- Discovery metadata
    producer_name TEXT,                    -- Normalized producer name
    discovered_via TEXT NOT NULL,          -- 'serp_search', 'manual', 'wine_reference'
    discovery_wine_id BIGINT,              -- Wine that led to discovery (optional FK)

    -- Verification status
    status TEXT NOT NULL DEFAULT 'pending'
        CHECK (status IN ('pending', 'verified', 'rejected', 'unreachable')),
    verified_at TIMESTAMPTZ,
    verification_method TEXT,              -- 'manual', 'auto_domain_match', 'known_producer'

    -- Crawl configuration
    crawl_enabled BOOLEAN NOT NULL DEFAULT false,
    crawl_priority INTEGER NOT NULL DEFAULT 5,  -- 1=highest, 10=lowest
    last_crawled_at TIMESTAMPTZ,
    next_crawl_after TIMESTAMPTZ,

    -- Timestamps
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_producer_domains_status
    ON producer_domains (status);
CREATE INDEX IF NOT EXISTS idx_producer_domains_crawl
    ON producer_domains (crawl_enabled, next_crawl_after)
    WHERE crawl_enabled = true;

-- =============================================================================
-- robots.txt Cache (RFC 9309 compliant)
-- =============================================================================
CREATE TABLE IF NOT EXISTS robots_txt_cache (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    domain TEXT NOT NULL UNIQUE,

    -- Content
    robots_txt_content TEXT,               -- Raw robots.txt content (null if 4xx or parse failed)

    -- RFC 9309 status
    fetch_status TEXT NOT NULL DEFAULT 'unknown'
        CHECK (fetch_status IN (
            'success',           -- 2xx: parsed and cached
            'not_found',         -- 4xx: ALLOW_ALL (no restrictions)
            'server_error',      -- 5xx: use cached if available
            'unreachable',       -- Network error/timeout
            'parse_error',       -- 2xx but couldn't parse
            'unknown'            -- Initial state
        )),
    http_status_code INTEGER,              -- Actual HTTP status code

    -- Parsed rules (JSONB for fast lookup)
    parsed_rules JSONB,                    -- { "userAgent": "*", "allow": [...], "disallow": [...], "crawlDelay": 1 }
    crawl_delay_seconds REAL DEFAULT 1.0,  -- Extracted crawl-delay or default

    -- Cache management
    fetched_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    expires_at TIMESTAMPTZ NOT NULL DEFAULT NOW() + INTERVAL '24 hours',  -- Hard 24h TTL per RFC 9309
    fetch_count INTEGER NOT NULL DEFAULT 1,
    last_error TEXT,

    -- Timestamps
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_robots_txt_cache_expires
    ON robots_txt_cache (expires_at);
CREATE INDEX IF NOT EXISTS idx_robots_txt_cache_domain
    ON robots_txt_cache (domain);

-- =============================================================================
-- Producer Crawl Queue (pending URLs to crawl)
-- =============================================================================
CREATE TABLE IF NOT EXISTS producer_crawl_queue (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    producer_domain_id BIGINT NOT NULL REFERENCES producer_domains(id) ON DELETE CASCADE,

    url TEXT NOT NULL,
    path TEXT NOT NULL,                    -- Extracted path for robots.txt check
    url_type TEXT NOT NULL DEFAULT 'other' -- 'wines', 'awards', 'downloads', 'range', 'press', 'other'
        CHECK (url_type IN ('wines', 'awards', 'downloads', 'range', 'press', 'accolades', 'medals', 'other')),

    -- Queue status
    status TEXT NOT NULL DEFAULT 'pending'
        CHECK (status IN ('pending', 'in_progress', 'completed', 'failed', 'blocked_by_robots')),
    priority INTEGER NOT NULL DEFAULT 5,   -- 1=highest, 10=lowest

    -- Crawl tracking
    attempts INTEGER NOT NULL DEFAULT 0,
    last_attempted_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    error_message TEXT,

    -- Result linking
    url_cache_id BIGINT REFERENCES public_url_cache(id),

    -- Timestamps
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    UNIQUE (producer_domain_id, url)
);

CREATE INDEX IF NOT EXISTS idx_crawl_queue_pending
    ON producer_crawl_queue (status, priority, created_at)
    WHERE status = 'pending';
CREATE INDEX IF NOT EXISTS idx_crawl_queue_domain
    ON producer_crawl_queue (producer_domain_id);

-- ============================================================
-- ROLLBACK (for reference - not auto-executed)
-- ============================================================
-- DROP TABLE IF EXISTS producer_crawl_queue;
-- DROP TABLE IF EXISTS robots_txt_cache;
-- DROP TABLE IF EXISTS producer_domains;
